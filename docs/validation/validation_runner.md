# Validation runner

`python -m simulation.validate` executes a set of seeded scenarios, copies outputs into isolated experiment folders, and applies consistency checks that layer on top of `simulation.verify`.

## Usage

```bash
python -m simulation.validate \
  --outdir simulation/experiments \
  --seed 22015001
```

Outputs are written to a timestamped directory under the chosen `--outdir` (for example `simulation/experiments/validation_20260101T120000`). Each scenario folder includes:

- `summary_stats.csv` and `tickets_stats.csv` copied from the run
- `config_used.json` capturing the effective configuration
- `verification_report.md` generated by `simulation.verify`
- `validation_report.md` and `validation_results.json` at the run root summarizing all checks

## Scenarios and overrides

The harness runs five scenarios with deterministic seeds derived from the base seed:

- **baseline** — current configuration
- **arrival_high** — increases `ARRIVAL_RATE` to apply queue pressure
- **feedback_high** — raises `FEEDBACK_P_DEV` and `FEEDBACK_P_TEST` to exercise feedback loops
- **service_slow** — scales every stage's service-time `scale` parameter by `1.5`
- **capacity_high** — increases `TOTAL_CONTRIBUTORS` to test capacity relief

Each run resets the configuration via `apply_config_overrides` and logs the applied overrides.

## Checks performed

1. **Verification integration** — reuses `simulation.verify` to build per-scenario verification reports.
2. **Bounds** — ensures waits, queue lengths, utilization, and closure rate stay within expected ranges.
3. **Conservation** — recomputes arrivals/closures against ticket rows, closure rate, throughput versus cycle counts, and Little-type identities (`avg_system ≈ avg_queue + avg_servers × utilization`).
4. **Baseline comparison** — compares reported metrics against `validation/baseline_metrics.csv` using a 10% relative tolerance when the baseline supplies a numeric expectation.
5. **Directionality/monotonicity** — checks that stress tests move in the expected direction (e.g., higher arrivals lead to larger waits/queue lengths/time-in-system; higher feedback increases waits and lowers closure rate; extra capacity lowers waits/utilization; slower service increases waits/time-in-system).

Failures appear in `validation_results.json` and the Markdown report; the CLI returns a non-zero status when any scenario or directionality check fails.
