# Codex Prompt Suite — 2026-01-03 (Validation Phase)

Purpose: end-to-end validation of the Apache BookKeeper workflow simulator against empirical behavior and consistency expectations.

## Prompt 1 — Establish Validation Charter & KPI Mapping
- **Goal:** Draft the validation plan that maps simulator metrics (closure_rate, throughput_*, avg_wait_*, avg_queue_length_*, utilization_*, rework_rate_*, markov_time_in_states, avg_servers_*, avg_system_length_*) to the real ASF BookKeeper workflow, defines what “real system” data is available (ETL exports), and sets acceptance tolerances/monotonic expectations.
- **Files to read:** `docs/CONCEPTUAL_WORKFLOW_MODEL.md`, `docs/key_metrics_3.2C.md`, `docs/analytical_model.md`, `docs/analytical_equations_3.2A.md`, `docs/stats/stats_py_evaluation_explained.md`, `docs/simulation/e2e_run_notes.md`, `docs/verification.md`, `meeting_logs/2025-12-28.md`, `simulation/config.py`, `simulation/stats.py`, `simulation/run_sweeps.py`, `simulation/verify.py`, `etl/output/*` (especially `csv/tickets_prs_merged.csv`, `csv/fit_summary.csv`).
- **Files to change/create:** Create `docs/validation/validation_plan.md` (new) plus a short index entry/link in `docs/project_documentation.md` if needed.
- **Objectives:**
  - Identify validation KPIs and tie them to observed quantities in ETL exports (arrival process, service fits, feedback rates, developer state occupancy). Clarify baseline horizon and units (days) and document how backlog maps to DEV queue metrics.
  - Define acceptance criteria: tolerances for matching empirical closure throughput, queue waits, utilization ranges, and monotonicity expectations (e.g., higher ARRIVAL_RATE → higher WIP/waits; higher FEEDBACK_P_TEST → higher total_wait/time_in_system; higher N_DEVS/avg_servers → lower waits/utilization). Include triggers for manual review when tolerances fail.
  - Describe what constitutes the “real system” proxy (historical ASF data vs. existing baseline runs) and when synthetic baselines are allowed.
  - Specify reproducibility requirements: required seeds (GLOBAL_RANDOM_SEED/BK_ASF_SIM_SEED), state parameter files, and output artifacts that must be retained for validation evidence.
- **Output artifacts:** `docs/validation/validation_plan.md` containing metric mappings, acceptance thresholds, monotonic expectations, data sources, and repeatability rules.
- **Definition of Done:** Plan file exists with concrete KPIs, tolerances, monotonic/consistency expectations, data provenance, and reproducibility notes; links added if needed; no implementation changes.

## Prompt 2 — Baseline Data & Empirical Alignment Prep
- **Goal:** Prepare validation-ready baselines by extracting empirical reference stats from ETL outputs and aligning them with simulator metrics for comparison.
- **Files to read:** ETL scripts/results (`etl/output/csv/tickets_prs_merged.csv`, `etl/output/csv/fit_summary.csv`, `etl/output/plots/*`), `etl/7_fit_distributions.py`, `etl/8_export_fit_summary.py`, `data/state_parameters/*`, `simulation/config.py`, `simulation/service_distributions.py`, `simulation/stats.py`, `docs/ETL_OVERVIEW_2.2C.md`.
- **Files to change/create:** New module `validation/baseline_extract.py`; optional helper config `validation/baseline_config.yaml`; documentation snippet appended to `docs/validation/validation_plan.md` summarizing produced baselines.
- **Objectives:**
  - Compute empirical aggregates matching simulator KPIs (arrival rate per day, service-time distribution summaries per stage, empirical feedback frequency, observed developer availability if present). Save as machine-readable (CSV/JSON) alongside provenance (data window, filters).
  - Export a “golden” baseline vector aligned to summary_stats metric names (closure_rate, throughput_*, avg_wait_*, avg_queue_length_*, utilization_*, rework_rate_*). Include confidence intervals if feasible.
  - Serialize the simulator configuration snapshot (ARRIVAL_RATE, FEEDBACK_P_*, service params, state parameter file hashes) used for comparison.
  - Ensure outputs are deterministic given fixed seeds and input files, and record hashes/versions to catch drift.
- **Output artifacts:** `validation/baseline_metrics.csv` (aligned to summary_stats metric names), `validation/baseline_metadata.json` (source files, hashes, date range, seeds), optional plots comparing empirical vs fitted distributions.
- **Definition of Done:** Reproducible scripts produce baseline metrics without modifying simulator behavior; baseline files live under `validation/`; plan updated with paths/provenance.

## Prompt 3 — Validation Harness & Consistency Checks
- **Goal:** Implement an automated validation runner that executes the simulator under controlled scenarios and checks behavioral consistency against the plan/baselines.
- **Files to read:** `simulation/simulate.py`, `simulation/run_sweeps.py`, `simulation/stats.py`, `simulation/workflow_logic.py`, `simulation/state_equations.py`, `simulation/verify.py`, `docs/verification.md`, `docs/stats/stats_py_evaluation_explained.md`, `docs/simulation/e2e_run_notes.md`, `validation/baseline_metrics.csv` (from Prompt 2).
- **Files to change/create:** New CLI module `simulation/validate.py` (or `validation/runner.py` invoked via entrypoint), helper library `validation/checks.py`, output templates under `validation/reports/`, documentation `docs/validation/validation_runner.md`; small additions to `setup` scripts or `README.md` only if needed to expose the command.
- **Objectives:**
  - Build seeded scenarios: (a) baseline run using current config; (b) stress tests varying ARRIVAL_RATE, FEEDBACK_P_TEST/DEV, service-time scale multipliers, and developer availability (avg_servers) using run_sweeps-style overrides; reuse experiment folders under `simulation/experiments/validation_*` to avoid overlap with other sweeps.
  - Implement consistency checks beyond verification: monotonicity (higher ARRIVAL_RATE ⇒ higher avg_wait/avg_queue_length/time_in_system; higher FEEDBACK_P_* ⇒ higher total_wait/time_in_system and lower closure_rate; increased avg_servers_* ⇒ lower waits/utilization), boundedness (utilization in [0,1], waits/non-negative times, closure_rate in [0,1]), and conservation (throughput ~= completions/horizon, avg_system_length ≈ avg_queue_length + avg_servers*utilization).
  - Compare simulation outputs to `validation/baseline_metrics.csv` with tolerances from Prompt 1; flag deviations with structured diagnostics. Integrate existing verification report when present to avoid duplicated calculations.
  - Emit machine-readable results (JSON/CSV) plus a Markdown validation report summarizing scenarios, expected directionality, observed metrics, pass/fail, and suggested remediation steps.
  - Provide deterministic seeding and log the exact config overrides used per scenario; ensure outputs are stored per experiment and not overwritten across runs.
- **Output artifacts:** CLI `python -m simulation.validate` (or equivalent) producing per-scenario `validation_report.md`, `validation_results.json`, and copied `summary_stats.csv`/`tickets_stats.csv` under `simulation/experiments/validation_*`.
- **Definition of Done:** Validation runner executes end-to-end locally (without external data), writes reports/artifacts per scenario, enforces monotonic/bound checks, integrates baseline comparisons, and returns non-zero exit on failures.

## Prompt 4 — Distribution & Parameter Plausibility Checks
- **Goal:** Add plausibility validators that compare simulator assumptions to empirical distributions and flag parameter drift.
- **Files to read:** `validation/baseline_metrics.csv`, `validation/baseline_metadata.json`, `etl/output/csv/fit_summary.csv`, `data/state_parameters/service_params.json`, `simulation/service_distributions.py`, `simulation/config.py`, `docs/DERIVATIONS_3.2A.md`, `docs/analytical_equations_3.2A.md`.
- **Files to change/create:** Extend `validation/checks.py` with distribution-comparison routines; optional notebook/script `validation/distribution_diagnostics.py`; document in `docs/validation/validation_runner.md`.
- **Objectives:**
  - Compare fitted service-time parameters in `config.py` / `service_params.json` to the latest ETL fits; flag deviations beyond configured tolerances (e.g., >5% change in lognormal scale/shape).
  - Sample simulator service distributions and compare empirical CDF/quantiles to ETL distributions; write plots/KS statistics where appropriate.
  - Validate arrival-rate magnitude against ETL arrival rates and ensure feedback probabilities stay within observed bounds.
  - Record any detected drift in validation reports with recommended actions (regenerate config, refresh ETL fits).
- **Output artifacts:** `validation/distribution_checks.json`, optional `validation/plots/service_distribution_comparison_*.png`, appended sections in validation report summarizing plausibility results.
- **Definition of Done:** Plausibility checks run within the validation CLI, produce human-readable diagnostics and machine-readable stats, and are deterministic/reproducible using stored ETL summaries.

## Prompt 5 — Validation Sweep Specs & Automation
- **Goal:** Define reproducible sweep specifications for validation scenarios and integrate them with the runner.
- **Files to read:** `simulation/sweeps/5_2B_sweeps.csv`, `docs/parameter_sweeps_5_2B.md`, `simulation/run_sweeps.py`, `simulation/verify.py`, `simulation/config.py`, `docs/verification.md`.
- **Files to change/create:** New sweep spec `simulation/sweeps/validation_monotonicity.csv`; optional helper `validation/sweep_layout.md`; minor adjustments to `simulation/run_sweeps.py` only if needed to support validation tags/output paths; update `docs/validation/validation_runner.md` with sweep usage.
- **Objectives:**
  - Encode validation scenarios (e.g., ARRIVAL_RATE multipliers, FEEDBACK_P_* variations, avg_servers overrides) as CSV spec compatible with `run_sweeps.py` plus validation runner hooks.
  - Ensure sweep outputs land under `simulation/experiments/validation_monotonicity/` with copied summary/ticket stats and validation reports aggregated.
  - Add an aggregate validation report summarizing all sweep experiments, including monotonic direction checks and deviations vs baseline metrics.
- **Output artifacts:** `simulation/sweeps/validation_monotonicity.csv`, experiment folders under `simulation/experiments/validation_monotonicity/`, consolidated `validation_sweep_report.md`.
- **Definition of Done:** Sweep spec executes via documented command, generates per-experiment validation artifacts, and aggregates pass/fail outcomes without clobbering existing sweeps.

## Prompt 6 — Reporting & Workflow Integration
- **Goal:** Integrate validation into the developer workflow and CI, ensuring reports are discoverable and repeatable.
- **Files to read:** `README.md`, `simulation/README.md`, `docs/simulation/e2e_run_notes.md`, `docs/project_documentation.md`, `reproducibility/*`, `SETUP_AND_USAGE_GUIDE.md`.
- **Files to change/create:** Update `README.md`/`simulation/README.md`/`docs/project_documentation.md` with validation entry points; add `Makefile`/script (`make validate` or `scripts/run_validation.sh`) if consistent with repo conventions; add CI hook/config if present (none yet).
- **Objectives:**
  - Document how to run validation (single run and sweep), required inputs (baseline files, state parameters), expected outputs, and failure semantics.
  - Provide a single command that runs the validation runner (and underlying verification) with fixed seeds and writes reports to a predictable location.
  - Outline how to archive validation artifacts for audit (e.g., copy reports into `output/validation/` with timestamps or hashes).
  - Ensure new commands do not interfere with existing verification harness; keep dependency footprint minimal (use existing requirements).
- **Output artifacts:** Updated docs with commands, optional `make validate` or shell script, and guidance for archiving artifacts.
- **Definition of Done:** Clear, actionable documentation and helper command exist; developers can reproduce validation locally; no breaking changes to existing workflows.
