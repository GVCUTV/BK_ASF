# Codex Prompt Suite — 2026-01-10

## Prompt 1 — Update the distribution fitting metric to use MSE instead of MAE.
**Goal**: Update the distribution fitting metric in `etl/7_fit_distributions.py` to use MSE instead of MAE.

**Context**:
- The script currently computes MAE between KDE and fitted PDF, stores it as `MAE_KDE_PDF`, labels fits as “Best-MAE (curve_fit)”, and selects the winner by lowest MAE.
- We need to switch to MSE everywhere in this script only.

**Files to Read**:
- `etl/7_fit_distributions.py`

**Objectives**:
1. Replace the MAE computation with MSE (mean squared error) between KDE and fitted PDF.
2. Update the metric name in the results rows from `MAE_KDE_PDF` to `MSE_KDE_PDF`.
3. Update `FitType` from “Best-MAE (curve_fit)” to “Best-MSE (curve_fit)”.
4. Update winner selection to use the MSE metric.
5. Update log messages to report “MSE” instead of “MAE”.

**Output Requirements**:
- Provide a unified diff for `etl/7_fit_distributions.py` only.

**Formatting Rules**:
- Preserve existing style, structure, and comments.
- Do not modify any other files.

**Definition of Done (DoD)**:
- The script computes MSE, logs MSE, outputs `MSE_KDE_PDF` in CSV, and selects the winner by lowest MSE.

## Prompt 2 — Analyze Distribution Change (Lognormal → Weibull)

**Goal**  
Determine why the distribution in `fit_summary.csv` for the DEV and TEST phases changed from lognormal to Weibull by analyzing the three consecutive commit diffs captured in `etl/commits_patches.txt`.

**Context**  
`etl/commits_patches.txt` contains the unified diffs of three consecutive commits. The change from lognormal to Weibull appears in `fit_summary.csv` for DEV and TEST phases. The objective is to trace which code or data processing changes in those commits caused the distribution selection to change.

**Files to Read**  
1. `etl/commits_patches.txt` (full file).  
2. `etl/output/csv/fit_summary.csv` (or the current location of the file in the repo if different).  
3. Every file referenced in the diffs that impacts distribution fitting, model selection, or summary CSV generation (e.g., ETL scripts, fitting modules, config files).

**Objectives**  
1. Parse the three commit diffs to identify all changes that could affect:  
   - Distribution fitting logic.  
   - Selection criteria between candidate distributions.  
   - Input data preprocessing for DEV/TEST phases.  
   - CSV output generation for `fit_summary.csv`.  
2. Map each relevant diff hunk to a plausible effect on distribution selection (e.g., parameter changes, data filtering, criteria thresholds).  
3. Determine which specific commit(s) introduced the change from lognormal to Weibull for DEV/TEST and explain the causal chain.  
4. Confirm whether the observed change is due to updated data, changed fitting criteria, or changed code paths.  
5. Produce a concise explanation with evidence tied to the exact diff hunks and file paths.

**Output Requirements**  
- Produce a single Markdown report with:  
  - **Summary of Findings** (what changed and why).  
  - **Commit-by-Commit Analysis** (cite the specific diff hunks responsible).  
  - **Conclusion** (the root cause of the lognormal → Weibull switch for DEV/TEST).  
- Include file paths and line references to the diff hunks from `etl/commits_patches.txt`.  
- Do **not** modify any files.  
- Do **not** run simulations.

**Formatting Rules**  
- One report only, in English.  
- No extra commentary outside the requested report.  
- Clearly identify commit boundaries as they appear in `etl/commits_patches.txt`.

**Definition of Done**  
- The report explains *why* the distribution changed for DEV/TEST, identifies the responsible commit(s), and ties the explanation to concrete diff evidence.  
- All files listed above were read.  
- No file modifications were made.

## Prompt 3 — Analyze Lognormal vs Weibull Service-Time Fit Mismatch

**Goal**  
Investigate (without changing the repository) why the simulation outputs / assumes a **lognormal** distribution for service times in **DEV** and **TEST** phases, while the ETL on collected data fits a **Weibull** distribution for both phases. Produce a full, in-depth, evidence-based analysis with concrete pointers to code paths, configurations, and statistical methodology that could explain the mismatch.

---

### Context
- We have two “truth sources” that disagree:
  1) **Simulation**: reports/uses **lognormal** service-time distribution for DEV and TEST.
  2) **ETL** over collected historical/service-time data: fits **Weibull** for both DEV and TEST.
- The analysis must be done **without modifying any repo file** (read-only investigation).
- The outcome must help the team decide *whether the simulation or ETL is wrong*, or whether both are “right” but operating on different definitions / data subsets.

---

### Files to Read
Codex must search and read all relevant files (do not assume names; locate them in-repo), especially:

**Simulation side**
- `simulation/**` (all python modules)
- `simulation/config/**` or any config folder
- `simulation/sweeps/**` (CSV sweep definitions, parameter overrides)
- `simulation/run*.py` entrypoints
- `simulation/stats.py` (how service times are recorded/aggregated/exported)
- Any file defining service-time distributions/parameters (e.g. `SERVICE_TIME_PARAMS`, `params.py`, `distributions.py`, `rng.py`, etc.)
- Any seed / RNG / sampling utilities

**ETL side**
- `etl/**`, `data_pipeline/**`, `scripts/**`, `notebooks/**` (wherever the ETL lives)
- Code that:
  - builds the service-time dataset per phase
  - cleans data (filters outliers, truncations, imputation)
  - fits distributions (Weibull vs lognormal)
  - selects the “best” distribution (AIC/BIC/KS/AD/LL, etc.)
  - exports parameters used by simulation (if any)
- Any exported artifacts (CSV/JSON) produced by ETL that are later consumed by simulation (e.g. `tickets_stats.csv`, `service_time_params.*`, etc.)

**Shared artifacts**
- Any docs or markdown describing the modeling assumptions and how service time is defined/collected
- Any raw or processed datasets used for distribution fitting (if present in repo)

---

### Objectives
Codex must produce an in-depth analysis that answers *at minimum*:

1) **What does the simulation actually do?**
   - Identify the exact code path where DEV and TEST service times are sampled.
   - Determine whether “lognormal” is:
     - hard-coded,
     - selected via a config flag,
     - selected because parameters exist only for lognormal,
     - or inferred from a “distribution name” field in an input artifact.
   - List the exact parameterization used (e.g., lognormal uses `(mu, sigma)` on log-scale; or mean/stdev transformed; location/scale; shift).
   - Verify whether service time sampling includes:
     - truncation/clipping,
     - minimum/maximum caps,
     - rounding/discretization,
     - mixture models,
     - conditional logic based on ticket type/priority/class.

2) **What does the ETL actually fit and why does it prefer Weibull?**
   - Identify the dataset used for fitting service times per phase (DEV and TEST).
   - Confirm the definition of “service time” used by ETL:
     - is it pure processing time?
     - elapsed time between status changes (which may include waiting/blocking)?
     - does it include pauses, weekends, batching, preemption?
   - Identify cleaning rules:
     - removing 0/negative durations,
     - handling missing timestamps,
     - outlier rejection,
     - censoring / truncation,
     - minimum granularity (seconds/minutes/hours).
   - Identify fitting approach:
     - MLE method/library,
     - goodness-of-fit tests (KS/AD/Chi-square),
     - information criteria (AIC/BIC),
     - how ties are broken.
   - Check whether ETL is fitting Weibull with/without location parameter (`loc`), and how that compares to simulation assumptions.

3) **Explain plausible mismatch causes, tied to evidence in the repo**
   Codex must investigate and explicitly confirm/deny each of the following common causes, citing the exact file/function lines:

   - **Different target variable**: simulation models “service time” but ETL is fitting “sojourn time” or “active+blocked time”.
   - **Censoring / truncation**: data collection cuts off long tasks, or ETL truncates tails; truncation can flip best-fit from lognormal to Weibull.
   - **Shifts (offsets)**: ETL uses `weibull_min` with `loc != 0` (shifted Weibull) while simulation assumes unshifted lognormal.
   - **Parameterization mismatch**: simulation’s “mu/sigma” computed from mean/stdev incorrectly (common bug) causing lognormal to be reported even if data came from Weibull.
   - **Model selection bug**: ETL comparison logic wrong (e.g., comparing log-likelihoods from different sample sizes, sign error, or mixing minimized vs maximized objective).
   - **Sampling source mismatch**: simulation reads parameters from an outdated artifact; ETL is newer (or vice versa).
   - **Phase mapping mismatch**: “DEV” in ETL corresponds to a different stage than “DEV” in simulation (e.g., includes review rework, merges dev+review).
   - **Mixture / heterogeneity**: data is a mixture of multiple classes; Weibull might win on aggregate while simulation uses a single-class lognormal (or vice versa).
   - **Time units mismatch**: ETL fits on seconds but simulation interprets as hours (or converts twice).
   - **RNG/seed artifacts**: simulation “reports” lognormal because configured, but actual sampled values may be transformed.
   - **Rounding/discretization**: ETL fits on discretized durations; discretization can bias toward Weibull-like shapes.
   - **Boundary conditions**: many near-zero values; Weibull shape parameter can capture hazard behavior; lognormal fit may degrade if zeros are present.

4) **Reproduce the decision trail (without changing repo)**
   - Identify where the “lognormal for dev/test” claim comes from:
     - printed logs,
     - a stats export,
     - a config report,
     - the sweep CSV,
     - or documentation.
   - Identify where the “weibull for dev/test” claim comes from in ETL outputs, and whether it’s per-run/per-slice or global.

5) **Actionable recommendations (but no code changes)**
   - Provide a prioritized list of what the team should do next to resolve the discrepancy, with minimal invasive steps:
     - e.g., run ETL on exactly the same subset/time window used to parametrize simulation,
     - ensure the ETL exports a single canonical `service_time_params` artifact and simulation reads it,
     - add a read-only diagnostic script or run command suggestion (but do not implement it).
   - Explicitly list what evidence would confirm each hypothesis.

---

### Output Requirements
Codex must produce:
- A **single in-depth analysis document** (markdown) written to:  
  `docs/analysis/service_time_distribution_mismatch.md`
- The document must include:
  - A “Repo evidence map” section: key files/functions and what they do
  - A “Simulation pipeline” section: how service times are sampled, transformed, and exported
  - An “ETL pipeline” section: how service times are constructed, cleaned, fitted, and selected
  - A “Mismatch hypotheses” section: each hypothesis, supporting evidence, and likelihood
  - A “Next steps” section: concrete experiments/checks to run (read-only), with expected outcomes
- **No repository modifications**:
  - No code edits, no refactors, no formatting changes, no config changes.
  - Only the creation of the markdown analysis file above is allowed.

---

### Formatting Rules
- Follow AGENTS.md
- Be explicit and precise: reference exact file paths and the exact functions/classes involved.
- Avoid vague statements. Every claim must be backed by a pointer to code/config/artifact behavior.
- If something cannot be determined from the repo contents, state it clearly and list what input is missing.

---

### Human Intervention
If any task cannot be automated:

## Prompt 4 — Fix 8_export_fit_summary.py after 7_fit_distributions.py changes
**Goal**: Fix `etl/8_export_fit_summary.py` so it correctly reads the updated distribution-fit output produced by `etl/7_fit_distributions.py`, eliminating the missing-column error and producing `fit_summary.csv` successfully.

**Context**:
- The script fails with: “Missing required columns {'Parametri', 'MAE_KDE_PDF', 'Distribuzione'} in ... distribution_fit_stats_review.csv” after `7_fit_distributions.py` changed its output schema.
- Use the latest output columns from `7_fit_distributions.py` to make `8_export_fit_summary.py` robust to the new headers, while preserving existing behavior for other stages/files when possible.
- Follow repository instructions in `AGENTS.md`.

**Files to Read**:
- `etl/7_fit_distributions.py`
- `etl/8_export_fit_summary.py`
- Any config/constants files those modules import that define output paths or column names.

**Objectives**:
1. Inspect `7_fit_distributions.py` to identify the exact column names written to the distribution-fit CSVs.
2. Update `8_export_fit_summary.py` to use the new column names (or add a compatible mapping layer) so it can read both the new schema and any legacy schema if present.
3. Ensure the winner-selection logic still uses the intended metrics (MAE_KDE_PDF / AIC / BIC / params) based on the new column names and formatting.
4. Keep changes scoped to `etl/8_export_fit_summary.py` unless absolutely necessary; if `7_fit_distributions.py` must be changed, request clarification first.
5. If running the script requires real datasets/paths not available, insert the required user prompt per instructions.

**Output Requirements**:
- Provide a unified diff for only the modified file(s).
- No additional commentary outside the diff.

**Formatting Rules**:
- Follow `AGENTS.md` and existing code style.
- Do not introduce new dependencies.

**Definition of Done (DoD)**:
- `etl/8_export_fit_summary.py` handles the new columns from `7_fit_distributions.py` without raising the missing-columns error.
- The output CSV still includes the correct “winner” distribution per stage using the intended metrics.
- If execution validation cannot be done due to missing data, insert the required user prompt.
